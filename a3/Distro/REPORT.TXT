CSC D84 - Artificial Intelligence

Assignment 3 - Reinforcement Learning - Q Learning

This assignment is worth:

10 AIUs (Artificial Intelligence Units)
toward the assignment component of your final
mark.

________________________________________________

Student Name 1 (last, first): Yang, Zongye

Student Name 2 (last, first):

Student number 1: 1004453423

Student number 2:

UTORid 1: yangzon7

UTORid 2:

READ THIS AND SIGN YOUR NAME AT THE END:

 I certify that I have read the UTSC code on academic
honesty and plaguarism. All work submitted as part
of this assignment is my own.

	Signed: _Zongye Yang__	_(student 2 name here)__


(-5 marks for failing to provide the identifying
 information requested above)
________________________________________________

Answer the following questions. Be concise and clear
but explain carefully when needed.

1 .- (1 mark) Explain your reward function. What game elements
      are used, and why you think this is a good reward function
      for this problem.
     The reward function takes into account whether the current state
     is a terminal node, such as when mouse is on the cheese, it returns the max
     reward. When the cat is on the mouse, it returns the minimum reward.
     Otherwise, the reward function returns 0 so it still needs to be explored.

2 .- These are multiple experiments (once you are sure your 
     QLearning code is working!) 

     * IMPORTANT! * For each training run, SAVE the Qtable
     you will re-use the Qtables later.

     (1 mark) # Experiment 1, 10000 training trials, 20 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 6.3%

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate: 74.2%

     (1 mark) # Experiment 2, 1000000 training trials, 50 rounds, random seed 1522,
     # 8x8 grid, cat-smartness .9

     Initial mouse winning rate (first rate obtained when training starts): 6.6%

     Train your mouse, and once trained, run the evaluation and  
     record the mouse winning rate: 91.2%

     (1 mark) Would the mouse keep improving and become invincible if
     you ran, say, 100,000,000 training rounds per batch?
     It's hard to tell. While I think the results would improve a bit, the
     mouse would never truly become invincible, certain layouts of the maze makes it
     hard for the mouse to win, and the behaviour of the cat is still an uncertain
     variable.

4 .- Using the QTable saved from Experiment 2 (NO re-training!)

     (1 mark) # Experiment 3 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 4289 for the game.
	
     Mouse Winning Rate: 40%

     (2 mark) # Experiment 4 - Run the evaluation on the 8x8 grid, cat-smartness=.9,
     # but using the random seed 31415 for the game.
	
     Mouse Winning Rate: 51%

     Average rate for Experiement 3 and Experiment 4: 40.7% and 51% respectively

     (1 mark) Compare with the rate obtained in experiment 2. What is happening here?
     The QLearning was trained on the maze layout in experiment 2. But when the maze
     changes and the mouse is in a new environment, the policy that the mouse has does
     not apply for the new environment. The mouse should be trained again on the new seed.

5 .- Understanding the effects of state space on learning ability:

     (1 mark) # Experiment 5 - Train your mouse on the 16x16 grid, using random seed
     # 1522, 1000000 trials, and 50 training rounds. Cat-smartness=.9

     Initial mouse winning rate (first rate obtained when training starts): 2.9%

     Mouse Winning Rate (from evaluation after training): 47.8%

     (1 mark) Compare the final winning rate with that from Experiment 2, what can
     you say about the difference in winning rates?
     The winning rate here is much lower than the one from Experiment 2. I can say that
     the difference in winning rates here is due to the state space difference between
     16x16 grid and 8x8 grid. Since there are many more states here, we need more rounds
     and more moves per round to explore more states and actions and generate
     a good policy for the mouse.

6 .- (2 marks) Is standard Q-Learning a reasonable strategy for environments
     that change constantly? discuss based on the above
     I do not think standard Q-Learning is a reasonable strategy for environments that change constantly
     such as this cat and mouse maze game. This is because key features like mouse location, cat location, wall
     locations change variably between different seeds and this could cause a mouse with an outdated policy to
     do poorly on the new one, as we saw with the difference in winning rate when we change the seed but keep the
     old QTable from experiment 2. Standard Q-Learning works best for static environments.


7 .- (3 marks) Explain your feature set for feature-based Q Learning, and
               explain why and how each feature is expected to contribute
	       to helping your mouse win

     1. the first feature is the total sum of the distances between the mouse and all
     cheeses. This helps our mouse to win because it indicates to the mouse which 
     moves move it closer to its goal of eating the cheeses and also if the mouse moves
     further away from the cheeses thereby getting further away from its goal. 

     2. the second feature the total sum of the distances between the mouse and all
     cats. This helps our mouse to win because it indicates to the mouse which 
     moves move it further away from cats so it can stay alive longer and attempt to
     eat the cheese vs if it moves closer to the cats thereby increasing its chances 
     of being eaten.

8 .- Carry out the following experiments:

     (1 mark) # Experiment 6: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, one cat, and one cheese, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts): 0.033125
     
     Mouse Winning Rate (from evaluation after training): 0.475564

     Compare this rate against the rate obtained in Experiment #5, what
     are your conclusions regarding the utility of feature-based
     Q-Learning?

     the winrate between the feature based Q-Learning and standard Q-Learning are very
     similar. this is because in this instance the state space for the 16x16 maze was small
     enough that standard was able to keep up, but over time, because we have a lot less
     computation in feature based, over time feature-based Q-Learning would outpace standard
     when it comes to learning and improving because it can iterate alot faster.

     (1 mark) # Experiment 7, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 4289, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 0.521448

     (1 mark) # Experiment 8, using the saved weights from Experiment 6 (no
     # re-training) run the evaluation using random seed 31415, same
     # 1 cheese, 1 cat, and cat-smartness=.9, 16x16 grid

     Mouse Winning Rate (from evaluation after training): 0.568401

     (1 mark) Comparing the winning rates from Experiments #7 and #8 against
     those from Experiment #6, what can you conclude about feature-
     based Q-Learning in terms of its ability to deal with changes 
     to the environment?
     the winning rates from experiments 7 and 8 are very consistent with the winning rate
     from experiment 6 which we used to train our weights. The results for all of them were
     pretty similar despite the fact that our wights had never seen the mazes in experiments
     6 and 7. This tells us that feature based q learning is good at adapting to changing 
     environments because its not learning specific configuration like regular Qlearning, but
     its learning based on abstracted features. because the same features can be calculated
     for any configuration, feature based q learning results in a more adaptible solution That
     perfoms consistently despite change. 
     
9 .- Carry out the following experiments:

     (1 mark) # Experiment 9: Train your mouse using feature-based Q-Learning, on
     # the 16x16 grid, 2 cats, and 3 cheeses, cat-smartness=.9,  
     # random seed = 1522, and use 1000000 trials and 50 rounds. 

     Initial mouse winning rate (first rate obtained when training starts): 0.080290
     
     Mouse Winning Rate (from evaluation after training): 0.274005
  
     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 16x16 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training):

     (1 mark) # Experiment 9: Using the trained weights from Experiment 9, (no
     # re-training), run the evaluation on the 32x32 grid, 2 cats, 
     # 3 cheeses, cat-smartness=.9, and random seed 4289
     
     Mouse Winning Rate (from evaluation after training): 0.270677

     (2 marks) Based on the last 3 experiments, what can you conclude regarding
     the usefulness and applicability of standard Q-Learning vs. 
     feature-based Q-learning?
     
     standard-Q-Learning is better when the state space is small and the environment is not constantly changing. this is because qlearning learns a specific configuration and needs a
     lot of memory so, if we have the computing power, or we need to stay in the same configuration then we will get better results using standard because we dont have to worry about 
     configuration changing and we can explore the entire state space easily.
     feature-based-Q-learning is better when the state space is really big or when there are changes or variability in the environement. this is because if the state space is too big
     standard Q-Learning becomes statistically impossible, rendering any solution useless. if there is a lot of variablility, then feature based is better because it learns on a set of
     abstracted features that be calculated for any environment, whereas standard learns for the good and bad states for a specific environment. in this scenario feature based will perform
     better and more consistently.

10 . - (2 marks) We obviously can not train an expensive robot by trial and error,
      How would you implement feature-based Q-learning for a mobile bot so you
      can take advantage of reinforcement learning but not spend millions on
      broken robots that keep falling down the stairs?
      we can use feature based Q-learning to train a mobile robot without having to actually use it in a real trial and error situation and risking it breaking.
      We can do so by picking a set of features that make sense for the robot/problem and them running the through q learning in a simlated environment. this allows us
      to still be able to perform qlearning and increase the performance of our weights without actually using it in a real trial and error situation. And feature based
      q learning is baesd on abstracted features we can expect similar and consistent performance of our bot when we eventually use it in the real world
      
      
_____________________________________________________

Mark with an 'x' where appropriate. If something is only
working partially, briefly describe what works, what
doesn't work, or what problems exist.
	
			Complete/Working	Partial		Not done

QLearn                x
 update

Reward                x
 function

Decide                x
 action

featureEval

evaluateQsa

maxQsa_prime

Qlearn_features

decideAction_features

_____________________________________________________

Marking:

(10 marks) Implemented QLearn update.

(5 marks) Implemented a reasonable reward function

(5 marks)  Implemented the function that chooses the
           optimal action based on Qtable and
           maze

(15 marks) Implemented a non-trivial, good feature set
	   and evaluation function

(10 marks) Implemented a working feature-based Q-learning
	   algorithm

(20 marks) Competitive mouse performance

(15 marks) Answers in this report file

(- marks)  Penalty marks

Total for A3:       / out of 80


